{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14268652,"sourceType":"datasetVersion","datasetId":9105512}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# import for train-test split\nfrom sklearn.model_selection import train_test_split\n\n# for features scaling\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\n# for confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# classification report\nfrom sklearn.metrics import classification_report\n\n# to calculate ROC-AUC, PR-AUC \nfrom sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n\n# for plots\nimport matplotlib.pyplot as plt\n\n# for precision, recall and f1 score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/cleaned-credit-card-fraud-dataset-for-ml/credit_card_cleaned.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:58:18.986018Z","iopub.execute_input":"2026-01-08T08:58:18.986439Z","iopub.status.idle":"2026-01-08T08:58:21.656529Z","shell.execute_reply.started":"2026-01-08T08:58:18.986418Z","shell.execute_reply":"2026-01-08T08:58:21.655383Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"As we read in Kaggke Machine Learning Course: \n\n**Building The Model**\n\nWe will use the scikit-learn library to create our models. When coding, this library is written as sklearn, as we will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.\n\n**The steps to building and using a model are: (DFPE)**\n\n**Define:**  What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n\n**Fit:**  Capture patterns from provided data. This is the heart of modeling.\n\n**Predict:**  Just what it sounds like\n\n**Evaluate:**  Determine how accurate the model's predictions are.","metadata":{}},{"cell_type":"markdown","source":"**Step 1: - Sanity Checks**","metadata":{}},{"cell_type":"code","source":"# Checks rows and columns\nprint(\"how many rows and columns are there: \")\nprint(df.shape)\nprint(\"\\n\")\n\n# Check which columns and their datatyoes are there\nprint(\"Columns and their datatypes: \")\ndf.info()\nprint(\"\\n\")\n\n# check nulls and their counts. we can display only first 5 record\nprint(\"Null counts: \")\ndf.isnull().sum().head()\n\n#top 5 rows of dataset\ndf.head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:58:41.443582Z","iopub.execute_input":"2026-01-08T08:58:41.443910Z","iopub.status.idle":"2026-01-08T08:58:41.501746Z","shell.execute_reply.started":"2026-01-08T08:58:41.443888Z","shell.execute_reply":"2026-01-08T08:58:41.500632Z"}},"outputs":[{"name":"stdout","text":"how many rows and columns are there: \n(283726, 31)\n\n\nColumns and their datatypes: \n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 283726 entries, 0 to 283725\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    283726 non-null  float64\n 1   V1      283726 non-null  float64\n 2   V2      283726 non-null  float64\n 3   V3      283726 non-null  float64\n 4   V4      283726 non-null  float64\n 5   V5      283726 non-null  float64\n 6   V6      283726 non-null  float64\n 7   V7      283726 non-null  float64\n 8   V8      283726 non-null  float64\n 9   V9      283726 non-null  float64\n 10  V10     283726 non-null  float64\n 11  V11     283726 non-null  float64\n 12  V12     283726 non-null  float64\n 13  V13     283726 non-null  float64\n 14  V14     283726 non-null  float64\n 15  V15     283726 non-null  float64\n 16  V16     283726 non-null  float64\n 17  V17     283726 non-null  float64\n 18  V18     283726 non-null  float64\n 19  V19     283726 non-null  float64\n 20  V20     283726 non-null  float64\n 21  V21     283726 non-null  float64\n 22  V22     283726 non-null  float64\n 23  V23     283726 non-null  float64\n 24  V24     283726 non-null  float64\n 25  V25     283726 non-null  float64\n 26  V26     283726 non-null  float64\n 27  V27     283726 non-null  float64\n 28  V28     283726 non-null  float64\n 29  Amount  283726 non-null  float64\n 30  fraud   283726 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.1 MB\n\n\nNull counts: \n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of             Time         V1         V2        V3        V4        V5  \\\n0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n...          ...        ...        ...       ...       ...       ...   \n283721  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n283722  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n283723  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n283724  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n283725  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n\n              V6        V7        V8        V9  ...       V21       V22  \\\n0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n...          ...       ...       ...       ...  ...       ...       ...   \n283721 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n283722  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n283723  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n283724  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n283725 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n\n             V23       V24       V25       V26       V27       V28  Amount  \\\n0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n...          ...       ...       ...       ...       ...       ...     ...   \n283721  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n283722  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n283723 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n283724 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n283725  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n\n        fraud  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n283721      0  \n283722      0  \n283723      0  \n283724      0  \n283725      0  \n\n[283726 rows x 31 columns]>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"**Dataset Readines:**\n\nThe dataset contains approximately 284,000 rows with a binary target column fraud.\nThere are no missing values, and all features are numeric, making the dataset suitable for machine learning.\nThe extreme imbalance in the target variable confirms that fraud detection is the core challenge of this problem.\n","metadata":{}},{"cell_type":"markdown","source":"**Step 2: - Verify Class Imbalance** (Crucial)","metadata":{}},{"cell_type":"code","source":"# this is core of the problem. count target values\ndf['fraud'].value_counts()\n\n# and normalize. Its the rate of occurrences of each value instead of the number of occurrences. \n#It returns the relative frequency by dividing all values by the sum of values. \n# rescaling real-valued numeric attributes into a 0 to 1 range\n\ndf['fraud'].value_counts(normalize= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:58:48.997820Z","iopub.execute_input":"2026-01-08T08:58:48.998119Z","iopub.status.idle":"2026-01-08T08:58:49.008237Z","shell.execute_reply.started":"2026-01-08T08:58:48.998096Z","shell.execute_reply":"2026-01-08T08:58:49.007525Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"fraud\n0    0.998333\n1    0.001667\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"The dataset is extremely imbalanced, with fraud transactions accounting for approximately 0.17% of all transacytions. This makes accuracy an unreliable evaluation metric and motivates the use of Precision-Recall curves and PR-AUC.","metadata":{}},{"cell_type":"markdown","source":"**Step 3: - Separate Features & Target**","metadata":{}},{"cell_type":"code","source":"# in general ML practice, 'capital x' is used for features and 'small y' for target variable\nX = df.drop(columns = ['fraud'])  \ny = df['fraud']\n\n#print(X)\n#print(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:58:54.655458Z","iopub.execute_input":"2026-01-08T08:58:54.655760Z","iopub.status.idle":"2026-01-08T08:58:54.676982Z","shell.execute_reply.started":"2026-01-08T08:58:54.655736Z","shell.execute_reply":"2026-01-08T08:58:54.676161Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"**Step 4: - Stratified Train/Test Split** (Very Important Step)","metadata":{}},{"cell_type":"code","source":"# remember: stratify = y for imbalanced data where some classes are underrepresented. This parameter \n# ensures that the training and testing have the same proportion of classes (or labels) as the original dataset\n\n# test_size=0.2: 20% data is for testing and 80% data for training\n\n# random_state = 42: Number 42 is just a \"seed\" number which used to set the seed for the random number generator. \n# Both random_state and seed are used to ensure that the same random numbers are generated every time the \n# code is run, which makes the results reproducible. It is not mandatory to use 42 only.\n\n# X_train, y_train are input and out for training data, X_test, y_test are input and output of test data \n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size = 0.2,\n    random_state = 42,\n    stratify = y\n)\n\n# normalize target values for test & train\n\nprint(y_train.value_counts(normalize = True))\nprint(y_test.value_counts(normalize = True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:59:03.012964Z","iopub.execute_input":"2026-01-08T08:59:03.013250Z","iopub.status.idle":"2026-01-08T08:59:03.169202Z","shell.execute_reply.started":"2026-01-08T08:59:03.013227Z","shell.execute_reply":"2026-01-08T08:59:03.167909Z"}},"outputs":[{"name":"stdout","text":"fraud\n0    0.998335\n1    0.001665\nName: proportion, dtype: float64\nfraud\n0    0.998326\n1    0.001674\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**Step 5: - Baseline Model (Logistic Regression)** ","metadata":{}},{"cell_type":"code","source":"# We scale data with StandardScaler to transform features to have a mean of 0 and a standard deviation of 1,\n# preventing features with larger scales from overpowering features with smaller scales in distance-based \n# models, ensuring all features contribute equally for better model accuracy and convergence\n\nscaler = StandardScaler()\n\n# scaling is crucial to prevent bias, ensures equal contribution, improves algorithm performance & aids gradient descent\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()    # // for getting value errors\n\ncols_to_scale = ['Time','Amount']\n\nX_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale].values)\n\n# fit(): The scaler calculates the necessary parameters (like the mean and standard deviation for each \n# column in the case of StandardScaler) from the training data only. transform(): It then applies these learned parameters to scale the training data. \n\n#Why use fit_transform() only on the training data?\n#The crucial point is that the scaling parameters must be learned only from the training data. \n# This prevents data leakage, where information about the test set's distribution might unintentionally \n# influence the model's training process. \n\nX_test_scaled[cols_to_scale]  = scaler.transform(X_test[cols_to_scale].values)\n\n# For the test data, you would use scaler.transform(X_test[cols_to_scale]) with the same scaler object ## \n# (which already \"learned\" the parameters from the training set). You do not call fit() again on the test data. ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:59:09.711136Z","iopub.execute_input":"2026-01-08T08:59:09.711412Z","iopub.status.idle":"2026-01-08T08:59:09.729354Z","shell.execute_reply.started":"2026-01-08T08:59:09.711390Z","shell.execute_reply":"2026-01-08T08:59:09.728604Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Step 6: - Train Baseline Logistic Regression**","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(\n    max_iter = 1000,\n    random_state = 42\n)\n\nlr.fit(X_train_scaled, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:59:21.827165Z","iopub.execute_input":"2026-01-08T08:59:21.827444Z","iopub.status.idle":"2026-01-08T08:59:22.694083Z","shell.execute_reply.started":"2026-01-08T08:59:21.827421Z","shell.execute_reply":"2026-01-08T08:59:22.693615Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(max_iter=1000, random_state=42)","text/html":"<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-2 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div> </div></div></div></div>"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"**Step 7: - Baseline Evaluation**","metadata":{}},{"cell_type":"markdown","source":"**Accuracy, Precision and Recall **\nThese three are used to evaluate the quality of classification model. \n\n**Accuracy** is how much classification model is correct overall. However if dataset is higly imbalance (as in credit card fraud detection case), then accuracy is not helpful metric. In this case we usr recall and precision. \n\n**Recall** shows if the ML model can find all he object of target class of all the positive samples in the dataset. In credit card fraud detection case recall should be higher, as we want to get maximum fraud detection. \n\n**Precision** shows how often the ML model ois correct when predicting the target class. it works well with imbalance data. it does not work well when the cost of false negative is high. so in case of credit card fraud detection precison should be lower and recall should be higher.  \n\n","metadata":{}},{"cell_type":"code","source":"# predictions & probabilities\ny_pred = lr.predict(X_test_scaled)\n\ny_prob = lr.predict_proba(X_test_scaled)[:, 1]\n# The predict_proba method returns an array with columns representing the predicted probabilities for each class\n# Each row corresponds to an instance in the test set, and the probabilities offer insights into the model's \n# confidence regarding the likelihood of each class.\n\n# cofusion matrix\n\nconfusion_matrix(y_test, y_pred)\n\n#A confusion matrix is a table that visualizes a classification model's performance by comparing \n#actual vs. predicted labels, showing correct (True Positives/Negatives) and incorrect (False Positives/Negatives) \n#predictions\n\n# classification report\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:59:30.254290Z","iopub.execute_input":"2026-01-08T08:59:30.254578Z","iopub.status.idle":"2026-01-08T08:59:30.289462Z","shell.execute_reply.started":"2026-01-08T08:59:30.254552Z","shell.execute_reply":"2026-01-08T08:59:30.288413Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56651\n           1       0.85      0.59      0.70        95\n\n    accuracy                           1.00     56746\n   macro avg       0.92      0.79      0.85     56746\nweighted avg       1.00      1.00      1.00     56746\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**Baseline Model Evaluation:**\n\nThe confusion matrix shows that the baseline logistic regression model identifies 56 fraudulent transactions (True Positives) while missing 39 fraudulent transactions (False Negatives) out of 95 total fraud cases.\n\nThis results in a fraud recall of approximately 59%, meaning that 41% of fraudulent transactions are missed, which is unacceptable in a real-world fraud detection system. \n\nAlthough the model reports near-perfect accuracy, this metric is misleading due to severe class imbalance.\n\nSince over 99.8% of transactions are non-fraudulent, a model can achieve very high accuracy while still failing to detect a significant proportion of fraud cases.\n\nNormal (0): 56,651  Fraud (1): 95  It means Extremely imbalanced (fraud is tiny).\n\nPrecision = 0.85\nWhen the model says “fraud”, it’s correct 85% of the time.\n\nRecall = 0.59\nIt catches 59% of real fraud cases.\nThat means it misses 41% of fraud.\n\n**Quick implication:**\nMissed frauds ≈ 95×(1−0.59)\nMissed frauds = 38.95 ≈ 39 fraud cases missed.","metadata":{}},{"cell_type":"markdown","source":"**Confusion Matrix + exact TP/FP/FN/TN**\n\nWe use a confusion matrix in machine learning because it gives a detailed breakdown of classification model performance, showing correct (True Positives/Negatives) and incorrect (False Positives/Negatives) predictions, which is much more insightful than just accuracy, especially with imbalanced datasets, allowing us to identify specific errors (like missed detections or false alarms) and calculate metrics like Precision, Recall, and F1-score to improve the model for different business needs. ","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T08:59:37.426677Z","iopub.execute_input":"2026-01-08T08:59:37.427037Z","iopub.status.idle":"2026-01-08T08:59:37.435874Z","shell.execute_reply.started":"2026-01-08T08:59:37.427017Z","shell.execute_reply":"2026-01-08T08:59:37.434524Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([[56641,    10],\n       [   39,    56]])"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"This matrix shows :\nTrue Positive (TP) : 56\nFalse Positive (FP) : 10\nFalse Negative (FN) : 39\nTrue Negative (TN) : 56641\n\nCaught Fraud: (Recall or True Positove Rate) = 56\nMissed fraud: False Negative (FN)  =  39\nTotal actual frauds = FN + TP = 39 + 56 = 95\n\n**Interpretation:**\n\nFraud recall = 0.86 approximately\n\nFraud precision = 0.046 approximately\n\nThe baseline logistic regression is conservative: when it flags fraud, it is usually correct (high precision), but it still misses a significant portion of fraudulent transactions (39 out of 95).\nAt default threshold of 0.5, the baseline logistic regression model detects 56 out of 95 fraud transactions while missing 39 fraud cases. Precision is high (approximately 85%), the recall is insufficient for a fraud detection system where missing frauds is costly.","metadata":{}},{"cell_type":"markdown","source":"\n**Precision and recall** are key metrics in machine learning classification, especially with imbalanced data, helping evaluate how well a model identifies relevant items versus avoiding false alarms. \n\n**Precision answers:** \"Of all items predicted positive, how many were actually positive?\" (minimizing false positives).\n\n**Recall answers:** \"Of all actual positive items, how many did the model find?\" (minimizing false negatives). They often involve a trade-off, with high recall potentially lowering precision, depending on the use case.\n\n\n\n\n\n**When to Use Which?**\n\n**High Precision Needed:** When the cost of a false positive is high (e.g., marking an important email as spam). \n\n**High Recall Needed:** When the cost of a false negative is high (e.g., missing a cancerous tumor or a weapon). \n\n**F1-Score:** A combined metric (harmonic mean of precision and recall) used for a balanced view, especially in imbalanced datasets.","metadata":{}},{"cell_type":"markdown","source":"**Step 8: Threshold Tuning**\n\nIt is used for decision making. We will see at what threshold we can catch more fraud while keeping false alarm reasonable.\n\n\n**For fraud detection we need to increase recall**\n\n**Acceptable tradeoff would be: lower precision is ok**\n\nwe will try multiple threshold now.\n","metadata":{}},{"cell_type":"code","source":"#import numpy as np\nthresholds = np.arange(0.05, 1.00, 0.05)\n\nrows = []\nfor thr in thresholds:\n    y_thr = (y_prob >= thr).astype(int)\n    prec = precision_score(y_test, y_thr, zero_division=0)\n    rec  = recall_score(y_test, y_thr)\n    f1   = f1_score(y_test, y_thr)\n    tn, fp, fn, tp = confusion_matrix(y_test, y_thr).ravel()\n    rows.append([thr, prec, rec, f1, fp, fn, tp])\n\ndf_thr = pd.DataFrame(rows, columns=[\"Threshold\",\"Precision\",\"Recall\",\"F1\",\"FP\",\"FN\",\"TP\"])\ndf_thr.sort_values([\"Recall\",\"Precision\"], ascending=[False, False]).head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T09:59:34.286505Z","iopub.execute_input":"2026-01-08T09:59:34.286835Z","iopub.status.idle":"2026-01-08T09:59:34.542905Z","shell.execute_reply.started":"2026-01-08T09:59:34.286807Z","shell.execute_reply":"2026-01-08T09:59:34.541708Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"    Threshold  Precision    Recall        F1  FP  FN  TP\n0        0.05   0.804348  0.778947  0.791444  18  21  74\n1        0.10   0.843373  0.736842  0.786517  13  25  70\n2        0.15   0.839506  0.715789  0.772727  13  27  68\n3        0.20   0.835443  0.694737  0.758621  13  29  66\n4        0.25   0.824324  0.642105  0.721893  13  34  61\n5        0.30   0.845070  0.631579  0.722892  11  35  60\n6        0.35   0.845070  0.631579  0.722892  11  35  60\n7        0.40   0.852941  0.610526  0.711656  10  37  58\n8        0.45   0.850746  0.600000  0.703704  10  38  57\n9        0.50   0.848485  0.589474  0.695652  10  39  56\n10       0.55   0.859375  0.578947  0.691824   9  40  55\n11       0.60   0.859375  0.578947  0.691824   9  40  55\n12       0.65   0.852459  0.547368  0.666667   9  43  52\n13       0.70   0.844828  0.515789  0.640523   9  46  49\n14       0.75   0.839286  0.494737  0.622517   9  48  47","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Threshold</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>FP</th>\n      <th>FN</th>\n      <th>TP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.05</td>\n      <td>0.804348</td>\n      <td>0.778947</td>\n      <td>0.791444</td>\n      <td>18</td>\n      <td>21</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10</td>\n      <td>0.843373</td>\n      <td>0.736842</td>\n      <td>0.786517</td>\n      <td>13</td>\n      <td>25</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.15</td>\n      <td>0.839506</td>\n      <td>0.715789</td>\n      <td>0.772727</td>\n      <td>13</td>\n      <td>27</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.20</td>\n      <td>0.835443</td>\n      <td>0.694737</td>\n      <td>0.758621</td>\n      <td>13</td>\n      <td>29</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.25</td>\n      <td>0.824324</td>\n      <td>0.642105</td>\n      <td>0.721893</td>\n      <td>13</td>\n      <td>34</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.30</td>\n      <td>0.845070</td>\n      <td>0.631579</td>\n      <td>0.722892</td>\n      <td>11</td>\n      <td>35</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.35</td>\n      <td>0.845070</td>\n      <td>0.631579</td>\n      <td>0.722892</td>\n      <td>11</td>\n      <td>35</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.40</td>\n      <td>0.852941</td>\n      <td>0.610526</td>\n      <td>0.711656</td>\n      <td>10</td>\n      <td>37</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.45</td>\n      <td>0.850746</td>\n      <td>0.600000</td>\n      <td>0.703704</td>\n      <td>10</td>\n      <td>38</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.50</td>\n      <td>0.848485</td>\n      <td>0.589474</td>\n      <td>0.695652</td>\n      <td>10</td>\n      <td>39</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.55</td>\n      <td>0.859375</td>\n      <td>0.578947</td>\n      <td>0.691824</td>\n      <td>9</td>\n      <td>40</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.60</td>\n      <td>0.859375</td>\n      <td>0.578947</td>\n      <td>0.691824</td>\n      <td>9</td>\n      <td>40</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.65</td>\n      <td>0.852459</td>\n      <td>0.547368</td>\n      <td>0.666667</td>\n      <td>9</td>\n      <td>43</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.70</td>\n      <td>0.844828</td>\n      <td>0.515789</td>\n      <td>0.640523</td>\n      <td>9</td>\n      <td>46</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.75</td>\n      <td>0.839286</td>\n      <td>0.494737</td>\n      <td>0.622517</td>\n      <td>9</td>\n      <td>48</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"**Table Results Interpretation:**\n\nIts clear from the table that as threshold decreases recall increases and precision decreases. It means fewer missed frauds and more false alarms. \n\nIn credit card fraud detection case, we cannot affoard missed fraud detection, the price of missed fraud detection is higher(lower recall) but we can accept increased false alarms(decreased precision).\n\nWe can see that at 0.05 threshold our recall is 0.78 and precision is 0.80 and F1(which is balance of recall and precision) is 0.79. It shows it can catch 78% of fraud detection. False positive and false negatives are 18 & 21 respectively.\n\nIts much better than threshold 0.5, where system could miss 59% frauds. Still its not desireable to accept this percentage of missing fraud detection. \n\nTo improve the model we will choose the threshold lower than 0.05 and will try to improve the performance.\n\nTo push recall higher we will test a finer grid like 0.001-0.05.","metadata":{}},{"cell_type":"code","source":"thresholds = np.arange(0.001, 0.051, 0.001)\n\nrows = []\nfor thr in thresholds:\n    y_thr = (y_prob >= thr).astype(int)\n    prec = precision_score(y_test, y_thr, zero_division=0)\n    rec  = recall_score(y_test, y_thr)\n    f1   = f1_score(y_test, y_thr)\n    tn, fp, fn, tp = confusion_matrix(y_test, y_thr).ravel()\n    rows.append([thr, prec, rec, f1, fp, fn, tp])\n\ndf_thr = pd.DataFrame(rows, columns=[\"Threshold\",\"Precision\",\"Recall\",\"F1\",\"FP\",\"FN\",\"TP\"])\ndf_thr.sort_values([\"Recall\",\"Precision\"], ascending=[False, False]).head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T10:00:02.951587Z","iopub.execute_input":"2026-01-08T10:00:02.951989Z","iopub.status.idle":"2026-01-08T10:00:03.631078Z","shell.execute_reply.started":"2026-01-08T10:00:02.951923Z","shell.execute_reply":"2026-01-08T10:00:03.630067Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"    Threshold  Precision    Recall        F1    FP  FN  TP\n1       0.002   0.045964  0.863158  0.087280  1702  13  82\n0       0.001   0.017730  0.863158  0.034746  4543  13  82\n2       0.003   0.078295  0.831579  0.143116   930  16  79\n6       0.007   0.313008  0.810526  0.451613   169  18  77\n5       0.006   0.229851  0.810526  0.358140   258  18  77\n4       0.005   0.164882  0.810526  0.274021   390  18  77\n3       0.004   0.120690  0.810526  0.210095   561  18  77\n7       0.008   0.372549  0.800000  0.508361   128  19  76\n30      0.031   0.735294  0.789474  0.761421    27  20  75\n29      0.030   0.728155  0.789474  0.757576    28  20  75\n28      0.029   0.721154  0.789474  0.753769    29  20  75\n26      0.027   0.714286  0.789474  0.750000    30  20  75\n27      0.028   0.714286  0.789474  0.750000    30  20  75\n25      0.026   0.707547  0.789474  0.746269    31  20  75\n24      0.025   0.694444  0.789474  0.738916    33  20  75","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Threshold</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>FP</th>\n      <th>FN</th>\n      <th>TP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.002</td>\n      <td>0.045964</td>\n      <td>0.863158</td>\n      <td>0.087280</td>\n      <td>1702</td>\n      <td>13</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.001</td>\n      <td>0.017730</td>\n      <td>0.863158</td>\n      <td>0.034746</td>\n      <td>4543</td>\n      <td>13</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003</td>\n      <td>0.078295</td>\n      <td>0.831579</td>\n      <td>0.143116</td>\n      <td>930</td>\n      <td>16</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.007</td>\n      <td>0.313008</td>\n      <td>0.810526</td>\n      <td>0.451613</td>\n      <td>169</td>\n      <td>18</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.006</td>\n      <td>0.229851</td>\n      <td>0.810526</td>\n      <td>0.358140</td>\n      <td>258</td>\n      <td>18</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.005</td>\n      <td>0.164882</td>\n      <td>0.810526</td>\n      <td>0.274021</td>\n      <td>390</td>\n      <td>18</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004</td>\n      <td>0.120690</td>\n      <td>0.810526</td>\n      <td>0.210095</td>\n      <td>561</td>\n      <td>18</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.008</td>\n      <td>0.372549</td>\n      <td>0.800000</td>\n      <td>0.508361</td>\n      <td>128</td>\n      <td>19</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.031</td>\n      <td>0.735294</td>\n      <td>0.789474</td>\n      <td>0.761421</td>\n      <td>27</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.030</td>\n      <td>0.728155</td>\n      <td>0.789474</td>\n      <td>0.757576</td>\n      <td>28</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.029</td>\n      <td>0.721154</td>\n      <td>0.789474</td>\n      <td>0.753769</td>\n      <td>29</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.027</td>\n      <td>0.714286</td>\n      <td>0.789474</td>\n      <td>0.750000</td>\n      <td>30</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.028</td>\n      <td>0.714286</td>\n      <td>0.789474</td>\n      <td>0.750000</td>\n      <td>30</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.026</td>\n      <td>0.707547</td>\n      <td>0.789474</td>\n      <td>0.746269</td>\n      <td>31</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.025</td>\n      <td>0.694444</td>\n      <td>0.789474</td>\n      <td>0.738916</td>\n      <td>33</td>\n      <td>20</td>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"**Threshold Tuning Explanation:**\n\nThreshold tuning was performed to explore the trade-off between fraud detection recall and false positives. As the classification threshold decreases, recall improves, reducing the number of missed frauds, but this comes at the cost of increased false alarms.\n\nAt very low thresholds (e.g., 0.001–0.002), recall reaches approximately 86%, but precision deteriorates sharply and the number of false positives becomes operationally prohibitive.\n\nA more balanced operating point is observed around thresholds 0.006–0.007, where recall remains above 80% while false positives are reduced by an order of magnitude compared to the lowest thresholds.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\n\nplt.plot(df_thr[\"Threshold\"], df_thr[\"Recall\"], label=\"Recall\")\nplt.plot(df_thr[\"Threshold\"], df_thr[\"Precision\"], label=\"Precision\")\n\n#plt.plot(df_thr[\"thresholds\"], df_thr[\"recall\"], label=\"Recall\")\n#plt.plot(df_thr[\"thresholds\"], df_thr[\"precision\"], label=\"Precision\")\nplt.xlabel(\"Thresholds\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"Precision–Recall vs Threshold\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T10:19:18.959755Z","iopub.execute_input":"2026-01-08T10:19:18.960096Z","iopub.status.idle":"2026-01-08T10:19:19.158422Z","shell.execute_reply.started":"2026-01-08T10:19:18.960072Z","shell.execute_reply":"2026-01-08T10:19:19.157554Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"Precision–Recall trade-off across low decision thresholds highlighting the operational region for improved fraud recall.","metadata":{}},{"cell_type":"markdown","source":"**Final Decision Statement:**\n\nBased on business considerations where missed fraud is significantly more costly than manual review of false positives, a decision threshold of 0.007 was selected.\nThis threshold provides a practical balance between fraud detection effectiveness and operational feasibility.","metadata":{}}]}